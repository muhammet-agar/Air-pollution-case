#First I read the file
pm0 <- read.table("RD_501_88101_1999-0.txt", comment.char = "#", header = FALSE, sep ="|", na.strings="")

#checking the dimensions
> dim(pm0)
[1] 117421     28

#checking the first 10 variables
> head(pm0)
  V1 V2 V3 V4 V5    V6 V7 V8  V9 V10      V11   V12
1 RD  I  1 27  1 88101  1  7 105 120 19990103 00:00
2 RD  I  1 27  1 88101  1  7 105 120 19990106 00:00
3 RD  I  1 27  1 88101  1  7 105 120 19990109 00:00
4 RD  I  1 27  1 88101  1  7 105 120 19990112 00:00
5 RD  I  1 27  1 88101  1  7 105 120 19990115 00:00
6 RD  I  1 27  1 88101  1  7 105 120 19990118 00:00
     V13  V14 V15 V16  V17 V18 V19 V20 V21 V22 V23 V24
1     NA   AS   3  NA <NA>  NA  NA  NA  NA  NA  NA  NA
2     NA   AS   3  NA <NA>  NA  NA  NA  NA  NA  NA  NA
3     NA   AS   3  NA <NA>  NA  NA  NA  NA  NA  NA  NA
4  8.841 <NA>   3  NA <NA>  NA  NA  NA  NA  NA  NA  NA
5 14.920 <NA>   3  NA <NA>  NA  NA  NA  NA  NA  NA  NA
6  3.878 <NA>   3  NA <NA>  NA  NA  NA  NA  NA  NA  NA
  V25 V26 V27 V28
1  NA  NA  NA  NA
2  NA  NA  NA  NA
3  NA  NA  NA  NA
4  NA  NA  NA  NA
5  NA  NA  NA  NA
6  NA  NA  NA  NA

#my variables don't have any names, so I'm gonna fetch them.
> cnames <- readLines("RD_501_88101_1999-0.txt", 1)
> cnames
#these are the names
[1] "# RD|Action Code|State Code|County Code|Site ID|Parameter|POC|Sample Duration|Unit|Method|Date|Start Time|Sample Value|Null Data Code|Sampling Frequency|Monitor Protocol (MP) ID|Qualifier - 1|Qualifier - 2|Qualifier - 3|Qualifier - 4|Qualifier - 5|Qualifier - 6|Qualifier - 7|Qualifier - 8|Qualifier - 9|Qualifier - 10|Alternate Method Detectable Limit|Uncertainty"

#now gotta fix the split
> cnames <- strsplit(cnames, "|", fixed=TRUE)
> cnames
[[1]]
 [1] "# RD"                             
 [2] "Action Code"                      
 [3] "State Code"                       
 [4] "County Code"                      
 [5] "Site ID"                          
 [6] "Parameter"                        
 [7] "POC"                              
 [8] "Sample Duration"                  
 [9] "Unit"                             
[10] "Method"                           
[11] "Date"                             
[12] "Start Time"                       
[13] "Sample Value"                     
[14] "Null Data Code"                   
[15] "Sampling Frequency"               
[16] "Monitor Protocol (MP) ID"         
[17] "Qualifier - 1"                    
[18] "Qualifier - 2"                    
[19] "Qualifier - 3"                    
[20] "Qualifier - 4"                    
[21] "Qualifier - 5"                    
[22] "Qualifier - 6"                    
[23] "Qualifier - 7"                    
[24] "Qualifier - 8"                    
[25] "Qualifier - 9"                    
[26] "Qualifier - 10"                   
[27] "Alternate Method Detectable Limit"
[28] "Uncertainty"

#now to put in the new variables names
> names(pm0) <- cnames[[1]]

#lets make the variables names a but more valid, without spaces etc.
> names(pm0) <- make.names(cnames[[1]])

#in this case I only need to look at the pm25 alue, as that is the air pollution I'm interested in.
> x0 <- pm0$Sample.Value
#put that into a vector

#checking some basics summaries of the new data x0
> str(x0)
 num [1:117421] NA NA NA 8.84 14.92 ...
> summary(x0)
   Min. 1st Qu.  Median    Mean 3rd Qu.    Max.    NA's 
   0.00    7.20   11.50   13.74   17.90  157.10   13217 
> mean(is.na(x0))
[1] 0.1125608
# interesting.. 11% of the data is missing.

#reading 2012 data
> pm1 <- read.table("RD_501_88101_2012-0.txt", comment.char = "#", header = FALSE, sep ="|", na.strings="")

#give the data some names again. I take the same names from the 1999 data, as it is the exact same variables just a different year
> names(pm1) <- make.names(cnames[[1]])

#now lets compare the basic statistics of the two datasets
> summary(x1)
   Min. 1st Qu.  Median    Mean 3rd Qu.    Max.    NA's 
 -10.00    4.00    7.63    9.14   12.00  908.97   73133 
> summary(x0)
   Min. 1st Qu.  Median    Mean 3rd Qu.    Max.    NA's 
   0.00    7.20   11.50   13.74   17.90  157.10   13217 

#we see that in the median, there's been a decrease of air pollution for the whole country. Good news!

#creating a boxplot to check on the data
> boxplot(x0, x1)

#you don't see a lot so let's take a different perspective
> boxplot(log10(x0), log10(x1))
Warning messages:
1: In boxplot.default(log10(x0), log10(x1)) : NaNs produced
2: In bplt(at[i], wid = width[i], stats = z$stats[, i], out = z$out[z$group ==  :
  Outlier (-Inf) in boxplot 1 is not drawn
3: In bplt(at[i], wid = width[i], stats = z$stats[, i], out = z$out[z$group ==  :
  Outlier (-Inf) in boxplot 2 is not drawn
  
# this much better once you run it and visualize.
#remember when using a log scale even the smallest changes and differences can have a large meaning.

#one interesting this is the negative number of pm2.5. Let's go deeper into this.
> negative <- x1 < 0
> str(negative)
 logi [1:1304287] FALSE FALSE FALSE FALSE FALSE FALSE ...
 
 > sum(negative, na.rm = TRUE)
[1] 26474
#this shows there are 26k values below 0. That's strange.

#checking to see the amount in precentage compared to the whole of x1
> mean(negative, na.rm = TRUE)
[1] 0.0215034
#2%!! that's not a lot.

#now checking to see which dates these negative values a recorded

> dates <- pm1$Date
> str(dates)
 int [1:1304287] 20120101 20120104 20120107 20120110 20120113 20120116 20120119 20120122 20120125 20120128 ...
 
 #dates in integers? that's not useful. Time to convert them.
> dates <- as.Date(as.character(dates), "%Y%m%d")
> str(dates)
 Date[1:1304287], format: "2012-01-01" "2012-01-04" "2012-01-07" ...
 
#Allright!! this looks great!!
#using a histogram, let's check for when there's most pm2.5 in 2012
> hist(dates, "month")

#lets now check for the negative amounts
> hist(dates[negative], "month")
#its mainly during the winter the negative measurements find place, so there might be a technical thing going on.
#as its only 2 % of the data, we do not worry that much about it, and move on. 


#Looking for a specific monitor now, that both existed in 1999 and 2012 from New York

> site0 <- unique(subset(pm0, State.Code == 36, c(County.Code, Site.ID)))
> site1 <- unique(subset(pm1, State.Code == 36, c(County.Code, Site.ID)))

#I created a new dataframe with the required information, County code and site ID

#creating a new variable that will paste country code and site id together
> site0 <- paste(site0[, 1], site0[, 2], sep = ".")
> site1 <- paste(site1[, 1], site1[, 2], sep = ".")

#doing it for both dataframes

#not it looks like this:
> str(site0)
 chr [1:33] "1.5" "1.12" "5.73" "5.80" "5.83" "5.110" "13.11" "27.1004" "29.2" ...
> str(site1)
 chr [1:18] "1.5" "1.12" "5.80" "5.133" "13.11" "29.5" "31.3" "47.122"
 
 #interestingly enough, the 1999 dataset has 33 combinations where county code and site ID match
 # now we want to see which county code and site id matches in both datasets, 1999 and 2012
 
 #I'm using intersect here and put them in the dataframe "both"
> both <- intersect(site0, site1)

#looking at both we can see there are 10 points that match
> both
 [1] "1.5"     "1.12"    "5.80"    "13.11"   "29.5"    "31.3"    "63.2008"
 [8] "67.1015" "85.55"   "101.3"
 
 #now we need one monitor that both matches the two periods, and has a lot of observations
 #So I put in the information I have into the real datasets
> pm0$county.site <- with(pm0, paste(County.Code, Site.ID, sep="."))
> pm1$county.site <- with(pm1, paste(County.Code, Site.ID, sep="."))

#and then I will subset that information and put it in a new dataframe, again only for New York state
> cnt0 <- subset(pm0, State.Code ==36 & county.site %in% both)
> cnt1 <- subset(pm1, State.Code ==36 & county.site %in% both)

#Now I want to split this dataframe to look at each individual monitor

> sapply(split(cnt0, cnt0$county.site), nrow)
   1.12     1.5   101.3   13.11    29.5    31.3    5.80 63.2008 67.1015   85.55 
     61     122     152      61      61     183      61     122     122       7 
# I can now see how many rows there are for each county in NY state
# county 1.12 (12 is for site ID) had 61 observations

#Doing it for both datasets
> sapply(split(cnt1, cnt1$county.site), nrow)
   1.12     1.5   101.3   13.11    29.5    31.3    5.80 63.2008 67.1015   85.55 
     31      64      31      31      33      15      31      30      31      31
#there's a difference in observations between each monitor 

#let's focus on a monitor - let's pick 63.2008 (county 68, site 2008).
> pm1sub <- subset(pm1, State.Code == 36 & County.Code == 63 & Site.ID == 2008)
> pm0sub <- subset(pm0, State.Code == 36 & County.Code == 63 & Site.ID == 2008)
> dim(pm1sub)
[1] 30 29
> dim(pm0sub)
[1] 122  29

#Time to play around with some dates
#Let's see if the levels of pm2.5 is going up or down in a certain period of time.
#I take the dates from the original dataset, and plot them into dates1
> dates1 <- pm1sub$Date
#I then take the values of pm2.5 and create a dataframe with the data
> x1sub <- pm1sub$Sample.Value

#looking at data vs pm2.5 plot, the dates don't look so good. They're in an integer format
> plot(dates1, x1sub)

#time to format the dates1
> dates1 <- as.Date(as.character(dates1), "%Y%m%d")

#now the plot looks a lot better
> plot(dates1, x1sub)

#doing the same for 1999 data
> dates0 <- pm0sub$Date
> x0sub <- pm0sub$Sample.Value
> dates0 <- as.Date(as.character(dates0), "%Y%m%d")
> plot(dates0, x0sub)

#now lets play around and try to compare the plots.
> plot(dates0, x0sub, pch =20)
> abline(h=median(x0sub, na.rm=T))
> plot(dates1, x1sub, pch =20)
> abline(h=median(x1sub, na.rm=T))

#there's a problem here. The Y-axis on the 1999 data starts at 0 and goes to 40
#while the Y-axis on 2012 dataset starts at 0 but goes to 14. Let's even this out.

> range(x0sub, x1sub, na.rm=T)
[1]  3.0 40.1
#we see here that the range goes from 3 to 40 for the datasets combined

#putting that into a dataframe now and plottig again
> rng <- range(x0sub, x1sub, na.rm=T)

> plot(dates0, x0sub, pch =20, ylim = rng)
> abline(h=median(x0sub, na.rm=T))
> plot(dates1, x1sub, pch =20, ylim = rng)
> abline(h=median(x1sub, na.rm=T))

#now the Y-axis for both are even

#the averages values are falling down, and the large spikes have gone as well. 

#Now let's go a step backwards. Let us take a look at different states.
#creating a value that takes the average pm2.5 for each state for the year 1999
#the tapply function takes care of this

> mn0 <- with(pm0, tapply(Sample.Value, State.Code, mean, na.rm = T))
> str(mn0)
 num [1:53(1d)] 19.96 6.67 10.8 15.68 17.66 ...
 - attr(*, "dimnames")=List of 1
  ..$ : chr [1:53] "1" "2" "4" "5" ...
  
  > summary(mn0)
   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. 
  4.862   9.519  12.315  12.406  15.640  19.956 
  
  #doing the same for 2012 data
  
 > mn1 <- with(pm1, tapply(Sample.Value, State.Code, mean, na.rm = T))
 > summary(mn1)
   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. 
  4.006   7.355   8.729   8.759  10.613  11.992
  
  #we already see a difference in means etc.
  #I need to create a data.frame for each year including the data State.Code and Sample.Value mean
  
  #now lets check the dataframe
  > head(d0)
  state      mean
1     1 19.956391
2     2  6.665929
4     4 10.795547
5     5 15.676067
6     6 17.655412
8     8  7.533304

#now lets merge these data frames by state name
> mrg <- merge(d0,d1, by = "state")

#checking to see the data
> dim(mrg)
[1] 52  3

> head(mrg)
  state    mean.x    mean.y
1     1 19.956391 10.126190
2    10 14.492895 11.236059
3    11 15.786507 11.991697
4    12 11.137139  8.239690
5    13 19.943240 11.321364
6    15  4.861821  8.749336

#mean.x is the 1999 data, mean.y is the 2012 data
#let's plot the data now
> par(mfrow = c(1,1))
> with(mrg, plot(rep(1999, 52), mrg[, 2], xlim = c(1998, 2013)))
> with(mrg, points(rep(2012, 52), mrg[, 3]))
 
#this looks promising, time to draw the line.
> segments(rep(1999, 52), mrg[,2], rep(2012, 52), mrg[,3])

#Annnnnd there you have it
#some states have gone up, some gone down
#some are off the chart, i forgot to add the ylim :/
#some states have barely moved at all


